{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0965284c-0a10-45af-a618-afad901472e5",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "592251ec-15f4-4799-8950-e313b6564df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bagdi\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:934: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' = Valkyria Chronicles III = \\n'}\n",
      "{'text': ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n'}\n",
      "{'text': \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"}\n",
      "{'text': \" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n\"}\n",
      "{'text': ' = = Gameplay = = \\n'}\n",
      "{'text': \" As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \\n\"}\n",
      "{'text': ' The game \\'s battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her \" Valkyria Form \" and become invincible , while Imca can target multiple enemy units with her heavy weapon . \\n'}\n",
      "{'text': \" Troops are divided into five classes : Scouts , Shocktroopers , Engineers , Lancers and Armored Soldier . Troopers can switch classes by changing their assigned weapon . Changing class does not greatly affect the stats gained while in a previous class . With victory in battle , experience points are awarded to the squad , which are distributed into five different attributes shared by the entire squad , a feature differing from early games ' method of distributing to different unit types . \\n\"}\n",
      "{'text': ' = = Plot = = \\n'}\n",
      "{'text': ' The game takes place during the Second Europan War . Gallian Army Squad 422 , also known as \" The Nameless \" , are a penal military unit composed of criminals , foreign deserters , and military offenders whose real names are erased from the records and thereon officially referred to by numbers . Ordered by the Gallian military to perform the most dangerous missions that the Regular Army and Militia will not do , they are nevertheless up to the task , exemplified by their motto , Altaha Abilia , meaning \" Always Ready . \" The three main characters are No.7 Kurt Irving , an army officer falsely accused of treason who wishes to redeem himself ; Ace No.1 Imca , a female Darcsen heavy weapons specialist who seeks revenge against the Valkyria who destroyed her home ; and No.13 Riela Marcellis , a seemingly jinxed young woman who is unknowingly a descendant of the Valkyria . Together with their fellow squad members , these three are tasked to fight against a mysterious Imperial unit known as Calamity Raven , consisting of mostly Darcsen soldiers . \\n'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "token = \"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\"\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\",use_auth_token=token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenizer_fn(examples):\n",
    "    encodings = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True,max_length=512)\n",
    "    encodings[\"labels\"] = encodings[\"input_ids\"].copy()\n",
    "    return encodings\n",
    "\n",
    "count = 0\n",
    "for example in dataset[\"train\"]:\n",
    "    if example[\"text\"].strip(): \n",
    "        count = count + 1\n",
    "        print(example)\n",
    "        if(count == 10):\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ebae589-e3a3-4564-bf0f-774277932cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1bcae6-4752-4cc1-bb56-8c330a0ca1e9",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f56a546b-add5-4a26-aca6-152689423f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13770' max='13770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13770/13770 2:19:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.433100</td>\n",
       "      <td>0.422343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>0.418494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.408500</td>\n",
       "      <td>0.417911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = dataset[\"train\"].map(tokenizer_fn, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_val = dataset[\"validation\"].map(tokenizer_fn, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_test = dataset[\"test\"].map(tokenizer_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=2,\n",
    "    fp16=True,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_train,eval_dataset=tokenized_val,data_collator=data_collator)\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7c0ce-0a05-4682-862f-1385cebdfa53",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca084a4e-4eaa-4bf0-abe3-d6e86c282a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1880' max='1880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1880/1880 02:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1.52\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from evaluate import load\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7780fed7-19ef-4f9e-a360-6b296a8b6603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import default_data_collator\n",
    "\n",
    "def evaluate_top_k_accuracy(model, dataset, k=5, batch_size=8):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device).eval()\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, collate_fn=default_data_collator)  \n",
    "    total_tokens = 0\n",
    "    matched_predictions = 0\n",
    "\n",
    "    for batch in tqdm(loader, desc=f\"Evaluating Top-{k} Accuracy\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attn_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        targets = input_ids[:, 1:]\n",
    "        inputs = input_ids[:, :-1]\n",
    "        mask = attn_mask[:, :-1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids=inputs, attention_mask=mask).logits  # [B, T, V]\n",
    "\n",
    "        top_k_indices = logits.topk(k=k, dim=-1).indices  # [B, T, K]\n",
    "        hits = (top_k_indices == targets.unsqueeze(-1)).any(dim=-1)  # [B, T]\n",
    "\n",
    "        matched_predictions += hits.sum().item()\n",
    "        total_tokens += hits.numel()\n",
    "\n",
    "    accuracy = matched_predictions / total_tokens if total_tokens else 0.0\n",
    "    return round(accuracy * 100, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12728d27-9f1a-4304-a0c7-81f87cd59c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Top-5 Accuracy: 100%|█████████████████████████████████████████████████████| 545/545 [02:08<00:00,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 Accuracy: 95.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "top5_acc = evaluate_top_k_accuracy(model, tokenized_test, k=5)\n",
    "print(f\"Top-5 Accuracy: {top5_acc}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3103021-f507-4b3b-89c9-4dfde7e717de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_model\\\\tokenizer_config.json',\n",
       " './fine_tuned_model\\\\special_tokens_map.json',\n",
       " './fine_tuned_model\\\\vocab.json',\n",
       " './fine_tuned_model\\\\merges.txt',\n",
       " './fine_tuned_model\\\\added_tokens.json',\n",
       " './fine_tuned_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467853e-5c3a-4094-b6e0-4ffded0261cf",
   "metadata": {},
   "source": [
    "# GUI INTERFACE (DO RUN IT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8165d498-258a-4a2f-b2bf-8cc652cf31be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import font as tkFont\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_model\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "window = tk.Tk()\n",
    "window.title(\"Next Word Predictor\")\n",
    "window.geometry(\"700x500\")\n",
    "window.configure(bg=\"#FFF3C7\")\n",
    "\n",
    "font_title = tkFont.Font(family=\"Comic Sans MS\", size=24, weight=\"bold\")\n",
    "font_label = tkFont.Font(family=\"Comic Sans MS\", size=16)\n",
    "font_button = tkFont.Font(family=\"Comic Sans MS\", size=12)\n",
    "\n",
    "header = tk.Label(window, text=\"Next Word Predictor\", font=font_title, fg=\"#FF6F61\", bg=\"#FFF3C7\")\n",
    "header.pack(pady=20)\n",
    "\n",
    "entry = tk.Entry(window, font=font_label, width=40, bg=\"#FFF6F1\", fg=\"#333\")\n",
    "entry.pack(pady=10)\n",
    "entry.focus_set()\n",
    "\n",
    "result_labels = []\n",
    "\n",
    "def get_top_k_predictions(text, k=3):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        top_k_ids = torch.topk(next_token_logits, k).indices[0].tolist()\n",
    "        predictions = [tokenizer.decode([i]).strip() for i in top_k_ids]\n",
    "    return predictions\n",
    "\n",
    "def on_change(event):\n",
    "    text = entry.get()\n",
    "    if not text.strip():\n",
    "        return\n",
    "    predictions = get_top_k_predictions(text)\n",
    "    for lbl in result_labels:\n",
    "        lbl.destroy()\n",
    "    result_labels.clear()\n",
    "    for pred in predictions:\n",
    "        lbl = tk.Label(window, text=pred, font=font_button, bg=\"#FFDD94\", fg=\"#000\", padx=10, pady=5, cursor=\"hand2\",\n",
    "                       relief=\"raised\", bd=2)\n",
    "        lbl.bind(\"<Button-1>\", lambda e, p=pred: (entry.insert(tk.END, \" \" + p.strip()), on_change(None)))\n",
    "        lbl.pack(pady=5)\n",
    "        result_labels.append(lbl)\n",
    "\n",
    "\n",
    "entry.bind(\"<KeyRelease>\", on_change)\n",
    "\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda7fbb-b0cf-40ba-990c-a49b271839c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
